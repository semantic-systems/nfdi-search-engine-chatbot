{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_df(path):\n",
    "  with open(path, 'r') as j:\n",
    "      contents = json.loads(j.read())\n",
    "\n",
    "  search_key, elapsed_time, publications, resources, others, results, result_lines, results_num = [], [], [], [], [], [], [], []\n",
    "  for result in contents:\n",
    "      # pprint(result)\n",
    "      search_key.append(result['search_key'])\n",
    "      elapsed_time.append(result['elapsed_time'])\n",
    "      # results.append(result['results'])\n",
    "      publications.append(result['stats']['publications'])\n",
    "      if 'resources' in result['stats']:\n",
    "          resources.append(result['stats'][\"resources\"])\n",
    "      else:\n",
    "          resources.append(None)\n",
    "      if 'others' in result['stats']:\n",
    "          others.append(result['stats'][\"others\"])\n",
    "      else:\n",
    "          others.append(None)\n",
    "      result_lines= []\n",
    "      num = 0\n",
    "      for line in result['results']:\n",
    "          result_lines.append(line)\n",
    "          num += 1\n",
    "      results.append(result_lines)\n",
    "      results_num.append(num)\n",
    "\n",
    "  df = pd.DataFrame([search_key, elapsed_time, publications, resources, others, results_num, results]).T\n",
    "  df = df.rename(columns={0: 'search_key', 1: 'elapsed_time', 2: 'publications', 3: 'resources', 4: 'others', 5: 'results_num', 6: 'results'})\n",
    "  return df, contents\n",
    "\n",
    "df, contents = json_to_df(\"search_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(275, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>search_key</th>\n",
       "      <th>elapsed_time</th>\n",
       "      <th>publications</th>\n",
       "      <th>resources</th>\n",
       "      <th>others</th>\n",
       "      <th>results_num</th>\n",
       "      <th>results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Motorcycle detection and tracking in ADAS</td>\n",
       "      <td>4.850967</td>\n",
       "      <td>76</td>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>110</td>\n",
       "      <td>[\\nMotorcycle detection for ADAS through camer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Exploring Large Language Models for Multilingu...</td>\n",
       "      <td>4.803219</td>\n",
       "      <td>140</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>174</td>\n",
       "      <td>[\\nExploring Large Language Models for Classic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Review on ICT for education of children with s...</td>\n",
       "      <td>3.701676</td>\n",
       "      <td>82</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>106</td>\n",
       "      <td>[\\nNORMLEX: Information System on Internationa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Scalable Semantic Similarity Estimation Framew...</td>\n",
       "      <td>4.095878</td>\n",
       "      <td>83</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>108</td>\n",
       "      <td>[\\nOntology Matching: A Machine Learning Appro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bag-of-Word and Bidirectional Attentive Memory...</td>\n",
       "      <td>3.68883</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>55</td>\n",
       "      <td>[\\nComparison on Large Language Model augmente...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          search_key elapsed_time  \\\n",
       "0          Motorcycle detection and tracking in ADAS     4.850967   \n",
       "1  Exploring Large Language Models for Multilingu...     4.803219   \n",
       "2  Review on ICT for education of children with s...     3.701676   \n",
       "3  Scalable Semantic Similarity Estimation Framew...     4.095878   \n",
       "4  Bag-of-Word and Bidirectional Attentive Memory...      3.68883   \n",
       "\n",
       "  publications resources others results_num  \\\n",
       "0           76        14     20         110   \n",
       "1          140        15     19         174   \n",
       "2           82         5     19         106   \n",
       "3           83         9     16         108   \n",
       "4           32         3     20          55   \n",
       "\n",
       "                                             results  \n",
       "0  [\\nMotorcycle detection for ADAS through camer...  \n",
       "1  [\\nExploring Large Language Models for Classic...  \n",
       "2  [\\nNORMLEX: Information System on Internationa...  \n",
       "3  [\\nOntology Matching: A Machine Learning Appro...  \n",
       "4  [\\nComparison on Large Language Model augmente...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response time analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, column_widths=[0.4, 0.6], \n",
    "                    horizontal_spacing = 0.04,  subplot_titles=(\"Retrieved Documents Numbers Distribution.\", \"Response Time Distribution\"))\n",
    "\n",
    "trace0 = go.Histogram(x=df['results_num'].tolist(),  showlegend=False)\n",
    "trace1 = go.Histogram(x=df['elapsed_time'].tolist(),  showlegend=False)\n",
    "\n",
    "\n",
    "fig.append_trace(trace0, 1, 1)\n",
    "fig.append_trace(trace1, 1, 2)\n",
    "\n",
    "fig.update_layout(height=600, width=1700, margin=dict(l=20, r=20, t=20, b=20))\n",
    "\n",
    "fig.write_image(\"response_time_results.pdf\", width=1300, height=520)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import recall_score\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_threshold_recall(similarities, threshold):\n",
    "    recalls = []\n",
    "    for sim in similarities:\n",
    "      filtered_sum = 0\n",
    "      for val in sim:\n",
    "        if val >= threshold:\n",
    "          filtered_sum += 1\n",
    "\n",
    "      recall = filtered_sum/len(sim)\n",
    "      recalls.append(recall)\n",
    "    return sum(recalls)/len(recalls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_cosine_scores(search_keys, documents):\n",
    "  similarities = []\n",
    "  # Calculate tfidf vectors\n",
    "  for key,res_texts in zip(search_keys, documents):\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    query_tfidf = tfidf_vectorizer.fit_transform([key]).toarray()\n",
    "    document_tfidf = tfidf_vectorizer.transform(res_texts).toarray()\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    cosine_similarities = cosine_similarity(query_tfidf, document_tfidf)\n",
    "    similarities.append(cosine_similarities[0])\n",
    "  return similarities\n",
    "\n",
    "df['tfidf_sim'] = get_tfidf_cosine_scores(df['search_key'], df['results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def get_bert_simlirarities(search_keys, documents):\n",
    "    similarities = []\n",
    "    for key,res_texts in tqdm(zip(search_keys, documents)):\n",
    "      # Compute embedding for both lists\n",
    "      embeddings1 = model.encode([res.lower() for res in res_texts], convert_to_tensor=True)\n",
    "      embeddings2 = model.encode([key.lower()], convert_to_tensor=True)\n",
    "\n",
    "      # Compute cosine-similarities\n",
    "      cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
    "      similarities.append(cosine_scores.reshape(-1).tolist())\n",
    "\n",
    "    return similarities\n",
    "\n",
    "df['bert_sim'] = get_bert_simlirarities(df['search_key'], df['results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "def get_bm25_similarities(search_keys, documents):\n",
    "  similarities = []\n",
    "  for key,res_texts in tqdm(zip(search_keys, documents)):\n",
    "    # corpus = \" \".join(res_texts)\n",
    "    corpus = [res.lower().split(\" \") for res in res_texts]\n",
    "    bm25 = BM25Okapi(corpus)\n",
    "\n",
    "    # Calculate BM25 scores for each search key\n",
    "    bm25_scores = []\n",
    "    bm25_scores = bm25.get_scores(key.lower().split(\" \"))\n",
    "    # bm25_scores.append(scores)\n",
    "\n",
    "    # Convert BM25 scores to cosine similarities\n",
    "    max_score = np.max(bm25_scores)\n",
    "    cosine_similarities = [np.array(scores) / max_score for scores in bm25_scores]\n",
    "    similarities.append(cosine_similarities)\n",
    "    # similarities.append(bm25_scores)\n",
    "  return similarities\n",
    "\n",
    "df['bm25_sim'] = get_bm25_similarities(df['search_key'], df['results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "thresholds = [th/100 for th in range(1, 100)]\n",
    "\n",
    "tfidf_plot_vals = []\n",
    "bert_plot_vals = []\n",
    "bm25_plot_vals = []\n",
    "for t in thresholds:\n",
    "    tfidf_plot_vals.append(get_threshold_recall(df['tfidf_sim'], t))\n",
    "    bert_plot_vals.append(get_threshold_recall(df['bert_sim'], t))\n",
    "    bm25_plot_vals.append(get_threshold_recall(df['bm25_sim'], t))\n",
    "\n",
    "f1 = go.Figure(\n",
    "    data = [\n",
    "        go.Scatter(x=thresholds, y=tfidf_plot_vals, name=\"TFIDF\"),\n",
    "        go.Scatter(x=thresholds, y=bert_plot_vals, name=\"BERT\"),\n",
    "        go.Scatter(x=thresholds, y=bm25_plot_vals, name=\"BM25\"),\n",
    "    ],\n",
    "    layout = {\"xaxis\": {\"title\": \"Thresholds\"}, \"yaxis\": {\"title\": \"Average Recall\"}}\n",
    ")\n",
    "\n",
    "f1.write_image(\"relevancy.pdf\", width=1300, height=520, scale=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"search_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
