{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2745d56f-b306-4eab-a45f-35576c6916c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "class NFDISearchDocumentProcessor:\n",
    "    IN_VALID_KEYS = [\"timedout_sources\"]\n",
    "    VALID_DATA_KEYS = [\n",
    "        \"name\",\n",
    "        \"author\",\n",
    "        # \"description\",\n",
    "        \"keywords\",\n",
    "        \"source\",\n",
    "        \"abstract\",\n",
    "        \"license\",\n",
    "        \"datePublished\",\n",
    "        \"dateModified\",\n",
    "        \"dateCreated\",\n",
    "        # \"inLanguage\",\n",
    "        \"publisher\",\n",
    "        \"orcid\",\n",
    "        \"affiliation\",\n",
    "        \"address\",\n",
    "        \"text\",\n",
    "    ]\n",
    "    VALID_DATA_KEY_LIST_DT = [\"inLanguage\", \"author\", \"keywords\"]\n",
    "\n",
    "    def process_single_doc(self, parent_topic: str, input_doc: Dict) -> str:\n",
    "        processed_doc = []\n",
    "        processed_doc_text = \"\"\n",
    "        for valid_key in self.VALID_DATA_KEYS:\n",
    "            value = input_doc.get(valid_key, \"NONE\")\n",
    "            if str(value).lower() != \"none\" and value != \"\":\n",
    "                if valid_key == \"author\":\n",
    "                    authors = \"\"\n",
    "                    for author in value:\n",
    "                        authors += \", \".join([item for item in author.values() if isinstance(item, str) and item.strip() and str(item) !=\"Person\"])\n",
    "\n",
    "                    value = authors\n",
    "                if valid_key == \"inLanguage\" or valid_key == \"keywords\":\n",
    "                    value = \", \".join(value)\n",
    "                if valid_key == \"source\":\n",
    "                    sources = \"\"\n",
    "                    try:\n",
    "                        for source in value:\n",
    "                            del source['identifier']\n",
    "                            sources += \", \".join([item for item in source.values() if isinstance(item, str) and item.strip()])\n",
    "                    except:\n",
    "                        pass\n",
    "                    value = sources\n",
    "                processed_doc_text += (\n",
    "                    f\"\\n{value}\"\n",
    "                )\n",
    "        return processed_doc_text\n",
    "\n",
    "    def process(self, items):\n",
    "        processed_docs = []\n",
    "        for parent_key, docs in items.items():\n",
    "            if parent_key not in self.IN_VALID_KEYS:\n",
    "                for index, doc in enumerate(docs):\n",
    "                    processed_doc_text = self.process_single_doc(\n",
    "                        parent_topic=parent_key, input_doc=doc\n",
    "                    )\n",
    "                    processed_docs.append(processed_doc_text)\n",
    "        processed_docs = list(set(processed_docs))     \n",
    "        # print(f\":::::::::::::::::::: Processed documents (NO:{len(processed_docs)}) ::::::::::::::\")\n",
    "        return processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "336c3956-6d65-4493-b9f7-4c103fef7100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.json  135.json  16.json   203.json  238.json  272.json  33.json  68.json\n",
      "101.json  136.json  170.json  204.json  239.json  273.json  34.json  69.json\n",
      "102.json  137.json  171.json  205.json  23.json   274.json  35.json  6.json\n",
      "103.json  138.json  172.json  206.json  240.json  275.json  36.json  70.json\n",
      "104.json  139.json  173.json  207.json  241.json  276.json  37.json  71.json\n",
      "105.json  13.json   174.json  208.json  242.json  277.json  38.json  72.json\n",
      "106.json  140.json  175.json  209.json  243.json  278.json  39.json  73.json\n",
      "107.json  141.json  176.json  20.json   244.json  279.json  3.json   74.json\n",
      "108.json  142.json  177.json  210.json  245.json  27.json   40.json  75.json\n",
      "109.json  143.json  178.json  211.json  246.json  280.json  41.json  76.json\n",
      "10.json   144.json  179.json  212.json  247.json  281.json  42.json  77.json\n",
      "110.json  145.json  17.json   213.json  248.json  282.json  43.json  78.json\n",
      "111.json  146.json  180.json  214.json  249.json  283.json  44.json  79.json\n",
      "112.json  147.json  181.json  215.json  24.json   284.json  45.json  7.json\n",
      "113.json  148.json  182.json  216.json  250.json  285.json  46.json  80.json\n",
      "114.json  149.json  183.json  217.json  251.json  286.json  47.json  81.json\n",
      "115.json  14.json   184.json  218.json  252.json  287.json  48.json  82.json\n",
      "116.json  150.json  185.json  219.json  253.json  288.json  49.json  83.json\n",
      "117.json  151.json  186.json  21.json   254.json  289.json  4.json   84.json\n",
      "118.json  152.json  187.json  220.json  255.json  28.json   50.json  85.json\n",
      "119.json  153.json  188.json  221.json  256.json  290.json  51.json  86.json\n",
      "11.json   154.json  189.json  222.json  257.json  291.json  52.json  87.json\n",
      "120.json  155.json  18.json   223.json  258.json  292.json  53.json  88.json\n",
      "121.json  156.json  190.json  224.json  259.json  293.json  54.json  89.json\n",
      "122.json  157.json  191.json  225.json  25.json   294.json  55.json  8.json\n",
      "123.json  158.json  192.json  226.json  260.json  295.json  56.json  90.json\n",
      "124.json  159.json  193.json  227.json  261.json  296.json  57.json  91.json\n",
      "125.json  15.json   194.json  228.json  262.json  297.json  58.json  92.json\n",
      "126.json  160.json  195.json  229.json  263.json  298.json  59.json  93.json\n",
      "127.json  161.json  196.json  22.json   264.json  299.json  5.json   94.json\n",
      "128.json  162.json  197.json  230.json  265.json  29.json   60.json  95.json\n",
      "129.json  163.json  198.json  231.json  266.json  2.json    61.json  96.json\n",
      "12.json   164.json  199.json  232.json  267.json  300.json  62.json  97.json\n",
      "130.json  165.json  19.json   233.json  268.json  301.json  63.json  98.json\n",
      "131.json  166.json  1.json    234.json  269.json  302.json  64.json  99.json\n",
      "132.json  167.json  200.json  235.json  26.json   30.json   65.json  9.json\n",
      "133.json  168.json  201.json  236.json  270.json  31.json   66.json\n",
      "134.json  169.json  202.json  237.json  271.json  32.json   67.json\n"
     ]
    }
   ],
   "source": [
    "ls ../assets/evaluation/search_results/raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf51783b-3637-4a19-9242-88d31e0cba8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def load_json(input_path):\n",
    "    with open(input_path, encoding=\"utf-8\") as f:\n",
    "        json_data = json.load(f)\n",
    "    return json_data\n",
    "\n",
    "def write_json(output_path, json_data):\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as outfile:\n",
    "        json.dump(json_data, outfile, indent=4, ensure_ascii=False)\n",
    "\n",
    "\n",
    "        \n",
    "def get_stats(data):\n",
    "    stats_dict = {group:len(items) for group, items in data.items() if group != 'timedout_sources' and len(items) != 0}\n",
    "    return stats_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95a8e921-5064-4f24-b87d-2d9b83fa2ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "raw_dir = \"../assets/evaluation/search_results/raw\"\n",
    "processed_dir = \"../assets/evaluation/search_results/processed\"\n",
    "keys = []\n",
    "processed_data = []\n",
    "for json_file in os.listdir(raw_dir):\n",
    "    if not json_file.startswith(\".\"):\n",
    "        input_path = os.path.join(raw_dir, json_file)\n",
    "        # output_path = os.path.join(processed_dir, json_file)\n",
    "        json_data = load_json(input_path)\n",
    "        json_data[0]['stats'] = get_stats(json_data[0]['results'])\n",
    "        keys += list(json_data[0]['stats'].keys())\n",
    "        json_data[0]['results'] = NFDISearchDocumentProcessor().process(json_data[0]['results'])\n",
    "        processed_data.append(json_data[0])\n",
    "\n",
    "write_json(os.path.join(processed_dir, \"search_results.json\"), processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ee40f9-782e-42c7-b97a-27f3e7923cf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe74824-381c-4794-b521-fefe9bc64121",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
